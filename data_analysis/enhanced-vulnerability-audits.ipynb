{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'enhanced-vulnerability-audits'\n",
    "\n",
    "# Add root to path\n",
    "import sys\n",
    "current_path = sys.path[0]\n",
    "root_name = 'data-lifecycle'\n",
    "root_path = current_path[:sys.path[0].find(root_name) + len(root_name)]\n",
    "if root_path not in sys.path:\n",
    "    sys.path.insert(0, root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import SVC\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from datasets import DatasetDict\n",
    "from common.directories import DATASET_DIR\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from plotter import Plotter\n",
    "\n",
    "plotter = Plotter(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = DatasetDict.load_from_disk(DATASET_DIR / DATASET_NAME)['train'].to_pandas()\n",
    "df_test = DatasetDict.load_from_disk(DATASET_DIR / DATASET_NAME)['test'].to_pandas()\n",
    "\n",
    "# concatenate train and test\n",
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reentrancy_df = df[df['type'].str.contains('reentrancy', case=False, na=False)]\n",
    "print(len(reentrancy_df))\n",
    "\n",
    "arithmetic_df = df[df['type'].str.contains('arithmetic', case=False, na=False)]\n",
    "print(len(arithmetic_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_size = min(50, len(reentrancy_df), len(arithmetic_df))\n",
    "combined_df = pd.concat([reentrancy_df.head(df_size), arithmetic_df.head(df_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_queries(queries, model, tokenizer):\n",
    "    encoded_queries = []\n",
    "    for query in queries:\n",
    "        inputs = tokenizer(query.replace(\"\\\\n\", \"\\n\"), return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Use the mean of the last hidden state as the sentence embedding\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        encoded_queries.append(embeddings)\n",
    "    return np.array(encoded_queries)\n",
    "\n",
    "def run_dim_reduction(df, field, model, tokenizer, title, plotname, Algo=PCA, annotate=False):\n",
    "    queries = df[field]\n",
    "    encoded_queries = encode_queries(queries, model, tokenizer)\n",
    "\n",
    "    # Apply dimensionality reduction\n",
    "    algo_result = Algo(n_components=2).fit_transform(encoded_queries)\n",
    "\n",
    "    # Separate the results into two clusters\n",
    "    cluster_1 = algo_result[:df_size]\n",
    "    cluster_2 = algo_result[df_size:]\n",
    "\n",
    "    # Prepare data for SVM\n",
    "    X = np.vstack((cluster_1, cluster_2))\n",
    "    y = np.array([0] * df_size + [1] * df_size)  # Create labels for the two clusters\n",
    "\n",
    "    # Fit a linear SVM\n",
    "    svm = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "    # Create a mesh to plot the decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the clusters and decision boundary\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['lightcoral', 'lightblue']))\n",
    "    plt.scatter(cluster_1[:, 0], cluster_1[:, 1], c='coral', label='Reentrancy vulnerability')\n",
    "    plt.scatter(cluster_2[:, 0], cluster_2[:, 1], c='mediumturquoise', label='Arithmetic error')\n",
    "\n",
    "    # Annotate points with query texts\n",
    "    if annotate:\n",
    "        for i, type in enumerate(df['type']):\n",
    "            plt.annotate(type, (algo_result[i, 0], algo_result[i, 1]))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.legend()\n",
    "    plt.grid(which=\"major\", linestyle='-', linewidth='0.5', color='gray')\n",
    "    plotter.save_plot(plotname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'description', model, tokenizer, 'PCA of Description Encodings by Uncased Base BERT', 'pca-descriptions-bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'description', model, tokenizer, 'PCA of Description Encodings by Uncased Large BERT', 'pca-descriptions-bert-large-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Base Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'description', model, tokenizer, 'PCA of Description Encodings by Cased Base BERT', 'pca-descriptions-bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Large Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained('bert-large-cased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'description', model, tokenizer, 'PCA of Description Encodings by Cased Large BERT', 'pca-descriptions-bert-large-cased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-large')\n",
    "model = AutoModel.from_pretrained(\"FacebookAI/xlm-roberta-large\")\n",
    "\n",
    "run_dim_reduction(combined_df, 'description', model, tokenizer, 'PCA of Description Encodings by RoBERTa', 'pca-descriptions-roberta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'code', model, tokenizer, 'PCA of Code Encodings by Uncased Base BERT', 'pca-code-bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'code', model, tokenizer, 'PCA of Code Encodings by Uncased Large BERT', 'pca-code-bert-large-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Base Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'code', model, tokenizer, 'PCA of Code Encodings by Cased Base BERT', 'pca-code-bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Large Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained('bert-large-cased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'code', model, tokenizer, 'PCA of Code Encodings by Cased Large BERT', 'pca-code-bert-large-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-large')\n",
    "model = AutoModel.from_pretrained(\"FacebookAI/xlm-roberta-large\")\n",
    "\n",
    "run_dim_reduction(combined_df, 'code', model, tokenizer, 'PCA of Code Encodings by RoBERTa', 'pca-code-roberta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'functionality', model, tokenizer, 'PCA of Functionality Encodings by Uncased Base BERT', 'pca-functionalities-bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'functionality', model, tokenizer, 'PCA of Functionality Encodings by Uncased Large BERT', 'pca-functionalities-bert-large-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Base Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'functionality', model, tokenizer, 'PCA of Functionality Encodings by Cased Base BERT', 'pca-functionalities-bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Large Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained('bert-large-cased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'functionality', model, tokenizer, 'PCA of Functionality Encodings by Cased Large BERT', 'pca-functionalities-bert-large-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained('bert-large-cased')\n",
    "\n",
    "run_dim_reduction(combined_df, 'code', model, tokenizer, 'TSNE of Code Encodings by Cased Large BERT', 'tsne-code-bert-large-cased', Algo=TSNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-large')\n",
    "model = AutoModel.from_pretrained(\"FacebookAI/xlm-roberta-large\")\n",
    "\n",
    "run_dim_reduction(combined_df, 'code', model, tokenizer, 'TSNE of Code Encodings by RoBERTa', 'tsne-code-roberta', Algo=TSNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained('bert-large-cased')\n",
    "\n",
    "run_dim_reduction(\n",
    "    combined_df,\n",
    "    'functionality',\n",
    "    model,\n",
    "    tokenizer,\n",
    "    'TSNE of Functionality Encodings by Cased Large BERT',\n",
    "    'tsne-functionality-bert-large-cased',\n",
    "    Algo=TSNE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-large')\n",
    "model = AutoModel.from_pretrained(\"FacebookAI/xlm-roberta-large\")\n",
    "\n",
    "run_dim_reduction(combined_df, 'functionality', model, tokenizer, 'TSNE of Functionality Encodings by RoBERTa', 'tsne-functionality-roberta', Algo=TSNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vulnearble vs Verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_df = df[df['type'].str.contains('no vulnerability', case=False, na=False)]\n",
    "print(len(verified_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([reentrancy_df.head(df_size), arithmetic_df.head(df_size), verified_df.head(df_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings(df, field, model, tokenizer, title, plotname, Algo=PCA, annotate=False):\n",
    "    queries = df[field]\n",
    "    encoded_queries = encode_queries(queries, model, tokenizer)\n",
    "\n",
    "    # Apply dimensionality reduction\n",
    "    algo_result = Algo(n_components=2).fit_transform(encoded_queries)\n",
    "\n",
    "    # Separate the results into two clusters\n",
    "    cluster_1 = algo_result[:df_size]\n",
    "    cluster_2 = algo_result[df_size:df_size*2]\n",
    "    cluster_v = algo_result[df_size*2:]\n",
    "\n",
    "    # Plot the clusters and decision boundary\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(cluster_1[:, 0], cluster_1[:, 1], c='limegreen', label='Reentrancy vulnerability')\n",
    "    plt.scatter(cluster_2[:, 0], cluster_2[:, 1], c='mediumturquoise', label='Arithmetic error')\n",
    "    plt.scatter(cluster_v[:, 0], cluster_v[:, 1], c='coral', label='Verified')\n",
    "\n",
    "    # Annotate points with query texts\n",
    "    if annotate:\n",
    "        for i, type in enumerate(df['type']):\n",
    "            plt.annotate(type, (algo_result[i, 0], algo_result[i, 1]))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.legend()\n",
    "    plt.grid(which=\"major\", linestyle='-', linewidth='0.5', color='gray')\n",
    "    plotter.save_plot(plotname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "compare_embeddings(combined_df, 'functionality', model, tokenizer, 'PCA of Functionality Encodings by Uncased Large BERT', 'pca-descriptions-bert-large-uncased-all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained('bert-large-cased')\n",
    "\n",
    "compare_embeddings(combined_df, 'functionality', model, tokenizer, 'PCA of Functionality Encodings by Cased Large BERT', 'pca-descriptions-bert-large-cased-all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained('bert-large-cased')\n",
    "\n",
    "compare_embeddings(combined_df, 'functionality', model, tokenizer, 'TSNE of Functionality Encodings by Cased Large BERT', 'tsne-descriptions-bert-large-cased-all', Algo=TSNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained('bert-large-cased')\n",
    "\n",
    "compare_embeddings(combined_df, 'code', model, tokenizer, 'PCA of Code Encodings by Cased Large BERT', 'pca-code-bert-large-cased-all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained('bert-large-cased')\n",
    "\n",
    "compare_embeddings(combined_df, 'code', model, tokenizer, 'TSNE of Code Encodings by Cased Large BERT', 'tsne-code-bert-large-cased-all', Algo=TSNE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
