{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/kza23/msc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIALECT_NAME = \"db_dialect\"\n",
    "\n",
    "csv.register_dialect(\n",
    "    DIALECT_NAME,\n",
    "    delimiter=\",\",\n",
    "    quoting=csv.QUOTE_MINIMAL,\n",
    "    escapechar=\"\\\\\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA A30 MIG 2g.12gb. Max memory: 11.688 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 20  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = (\n",
    "    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    ")\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "codeblocks = pd.read_csv(\"/vol/bitbucket/kza23/cleaned-up-code.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_verified = load_dataset(\n",
    "    \"msc-smart-contract-audition/vulnerable-functions-base\",\n",
    "    split=\"train\",\n",
    "    name=\"verified-functions\",\n",
    "    escapechar=\"\\\\\",\n",
    ")\n",
    "\n",
    "verified_df = dataset_verified.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            function\n",
      "0  ```\\nconstructor(string memory name_, string m...\n",
      "1  ```\\nconstructor(address[] memory tokensToList...\n",
      "2          ```\\nreceive() external payable {}\\n```\\n\n",
      "3  ```\\nfunction swap(\\n  address inputToken,\\n  ...\n",
      "4  ```\\nfunction addLiquidity(address inputToken,...\n"
     ]
    }
   ],
   "source": [
    "print(verified_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Below are provided 1 or more code blocks from a smart contract. The task is to explain what is its purpose in plain English.\n",
    "\n",
    "1. Do NOT use any names of variables, names of parameters or names of functions. Just try to explain the general functionality.\n",
    "2. Explain what each codeblock does step by step.\n",
    "3. Give a high-level overview and purpose of the code.\n",
    "4. First explain the functionality of each code block and after all codeblocks are explained provide a high-level overview of the code over all.\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Code block to explain:\n",
    "{}<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Code functionality:\n",
    "Code block 1:\n",
    "1. \"\"\"\n",
    "\n",
    "template_verified = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Below is a single codeblock from a smart contract. The task is to explain what is its purpose in plain English.\n",
    "\n",
    "1. Do NOT use any names of variables, names of parameters or names of functions. Just try to explain the general functionality.\n",
    "2. Explain the function does step by step.\n",
    "3. Give a high-level overview and purpose of the function within a wider context.\n",
    "4. First explain the functionality of the code block and after provide a high-level overview of the code over all.\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Code block to explain:\n",
    "{}<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Code functionality:\n",
    "Code block 1:\n",
    "1. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                      | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "@external\n",
      "@nonreentrant('lock')\n",
      "def remove_liquidity_one_coin(\n",
      "    token_amount: uint256, \n",
      "    i: uint256, \n",
      "    min_amount: uint256, \n",
      "    use_eth: bool = False, \n",
      "    receiver: address = msg.sender\n",
      ") -> uint256:\n",
      "    A_gamma: uint256[2] = self._A_gamma()\n",
      "\n",
      "    dy: uint256 = 0\n",
      "    D: uint256 = 0\n",
      "    p: uint256 = 0\n",
      "    xp: uint256[N_COINS] = empty(uint256[N_COINS])\n",
      "    future_A_gamma_time: uint256 = self.future_A_gamma_time\n",
      "    dy, p, D, xp = self._calc_withdraw_one_coin(A_gamma, token_amount, i, future_A_gamma_time > 0, True)\n",
      "    assert dy >= min_amount, \"Slippage\"\n",
      "\n",
      "    if block.timestamp >= future_A_gamma_time:\n",
      "        self.future_A_gamma_time = 1\n",
      "\n",
      "    self.balances[i] -= dy\n",
      "    CurveToken(self.token).burnFrom(msg.sender, token_amount)\n",
      "\n",
      "    coin: address = self.coins[i]\n",
      "    if use_eth and coin == WETH20:\n",
      "        raw_call(receiver, b\"\", value=dy)\n",
      "    else:\n",
      "        if coin == WETH20:\n",
      "            WETH(WETH20).deposit(value=dy)\n",
      "        response: Bytes[32] = raw_call(\n",
      "            coin, \n",
      "            _abi_encode(receiver, dy, method_id=\"transfer(address,uint256)\"), \n",
      "            max_outsize=32\n",
      "        )\n",
      "        if len(response)!= 0:\n",
      "            assert convert(response, bool)\n",
      "```\n",
      "```\n",
      "function _unstakeAndExitPool(\n",
      "    ICurve2TokenPool pool = ICurve2TokenPool(CURVE_POOL);\n",
      "    uint256[2] memory exitBalances = new uint256[](2);\n",
      "    if (isSingleSided) {\n",
      "        // Redeem single-sided\n",
      "        exitBalances[_PRIMARY_INDEX] = pool.remove_liquidity_one_coin(\n",
      "            poolClaim, int8(_PRIMARY_INDEX), _minAmounts[_PRIMARY_INDEX]\n",
      "        );\n",
      "    } else {\n",
      "        // Redeem proportionally, min amounts are rewritten to a fixed length array\n",
      "        uint256[2] memory minAmounts;\n",
      "        minAmounts[0] = _minAmounts[0];\n",
      "        minAmounts[1] = _minAmounts[1];\n",
      "\n",
      "        uint256[2] memory _exitBalances = pool.remove_liquidity(poolClaim, minAmounts);\n",
      "        exitBalances[0] = _exitBalances[0];\n",
      "        exitBalances[1] = _exitBalances[1];\n",
      "    }\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                      | 0/100 [00:19<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purpose: Code block 1:\n",
      "1.  This function is used to remove liquidity from a pool.\n",
      "2.  It takes in several parameters: the amount of tokens to remove, the index of the coin to remove, the minimum amount of tokens to remove, and an optional boolean flag to specify whether to use ETH or not.\n",
      "3.  The function calculates the amount of tokens to be removed (`dy`) and the amount of tokens to be transferred (`xp`) based on the pool's parameters and the amount of tokens to remove.\n",
      "4.  It then checks if the block timestamp is greater than or equal to the future A-gamma time. If it is, it updates the future A-gamma time.\n",
      "5.  The function then subtracts the amount of tokens to be removed from the pool's balance and burns the tokens from the sender's account.\n",
      "6.  If the use_eth flag is set to True, it uses the raw_call function to transfer the tokens to the receiver. Otherwise, it uses the raw_call function to transfer the tokens to the receiver, but only if the coin is WETH20. If the coin is not WETH20, it uses the WETH contract's deposit function to deposit the tokens.\n",
      "7.  The function also checks if the response from the raw_call function is not empty and asserts that the response is a boolean value.\n",
      "\n",
      "Code block 2:\n",
      "1.  This function is used to unstake and exit a pool.\n",
      "2.  It takes in a pool object and an array of minimum amounts.\n",
      "3.  If the pool is single-sided, it calls the remove_liquidity_one_coin function to remove liquidity from the pool and updates the exit balances.\n",
      "4.  If the pool is not single-sided, it calculates the minimum amounts for each coin and calls the remove_liquidity function to remove liquidity from the pool. It then updates the exit balances.\n",
      "\n",
      "High-level overview:\n",
      "The code is part of a smart contract that manages a liquidity pool. The contract allows users to remove liquidity from the pool and transfer the tokens to another account. The code includes functions to calculate the amount of tokens to be removed, update the pool's balance, and transfer the tokens. The contract also includes a function to unstake and exit the pool, which can be used to remove liquidity from the pool and transfer the tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 100\n",
    "current_chunk = 0\n",
    "start = current_chunk * chunk_len\n",
    "end = (current_chunk + 1) * chunk_len\n",
    "\n",
    "with open(f\"functionality-{current_chunk}.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, dialect=DIALECT_NAME)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow([\"functionality\"])\n",
    "\n",
    "    for codeblock in tqdm(codeblocks['code'][start:end], total=chunk_len):\n",
    "\n",
    "        print(codeblock.replace(\"\\\\n\", \"\\n\"))\n",
    "\n",
    "        prompt = template.format(codeblock.replace(\"\\\\n\", \"\\n\"))\n",
    "\n",
    "        input = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "\n",
    "        output_tokens = model.generate(\n",
    "            **input, max_new_tokens=512, pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        decoded_output_purpose = tokenizer.decode(\n",
    "            output_tokens[0],\n",
    "            skip_special_tokens=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        functionality = decoded_output_purpose.split(\"Code functionality:\\n\")[1].strip()\n",
    "        writer.writerow([functionality.replace(\"\\n\", \"\\\\n\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len = 100\n",
    "current_chunk = 0\n",
    "start = current_chunk * chunk_len\n",
    "end = (current_chunk + 1) * chunk_len\n",
    "\n",
    "with open(f\"functionality-verified-{current_chunk}.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, dialect=DIALECT_NAME)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow([\"functionality\"])\n",
    "\n",
    "    for codeblock in tqdm(verified_df['function'][start:end], total=chunk_len):\n",
    "\n",
    "        print(codeblock.replace(\"\\\\n\", \"\\n\"))\n",
    "        prompt = template_verified.format(codeblock.replace(\"\\\\n\", \"\\n\"))\n",
    "\n",
    "        input = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "\n",
    "        output_tokens = model.generate(\n",
    "            **input, max_new_tokens=512, pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        decoded_output_purpose = tokenizer.decode(\n",
    "            output_tokens[0],\n",
    "            skip_special_tokens=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        functionality = decoded_output_purpose.split(\"Code functionality:\\n\")[1].strip()\n",
    "        writer.writerow([functionality.replace(\"\\n\", \"\\\\n\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../functionality-verified-0.csv\n",
      "Processing ../functionality-verified-1.csv\n",
      "Processing ../functionality-verified-2.csv\n",
      "Processing ../functionality-verified-3.csv\n",
      "Processing ../functionality-verified-4.csv\n",
      "Processing ../functionality-verified-5.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "DIALECT_NAME = \"db_dialect\"\n",
    "csv.register_dialect(\n",
    "    DIALECT_NAME,\n",
    "    delimiter=\",\",\n",
    "    quoting=csv.QUOTE_MINIMAL,\n",
    "    escapechar=\"\\\\\",\n",
    ")\n",
    "\n",
    "# Output file name\n",
    "output_file = 'functionality-verified.csv'\n",
    "header = True\n",
    "# Open the output file in write mode to create/overwrite it\n",
    "with open(output_file, 'w') as outfile:\n",
    "    # Iterate through each chunk file\n",
    "    for i in range(6):\n",
    "        input_file = f'../functionality-verified-{i}.csv'\n",
    "        print(f\"Processing {input_file}\")\n",
    "\n",
    "        # Read the input CSV file\n",
    "        df = pd.read_csv(input_file, dialect=DIALECT_NAME)\n",
    "\n",
    "        # For the first file, include the header\n",
    "        df.to_csv(output_file, mode='a', index=False, header=header)\n",
    "        header = False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
