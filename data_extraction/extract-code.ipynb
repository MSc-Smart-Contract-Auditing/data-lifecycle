{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unsloth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unsloth'"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m DIALECT_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb_dialect\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mregister_dialect(\n\u001b[1;32m      4\u001b[0m     DIALECT_NAME,\n\u001b[1;32m      5\u001b[0m     delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mcsv\u001b[38;5;241m.\u001b[39mQUOTE_NONE,\n\u001b[1;32m      7\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "DIALECT_NAME = \"db_dialect\"\n",
    "\n",
    "csv.register_dialect(\n",
    "    DIALECT_NAME,\n",
    "    delimiter=\",\",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    escapechar=\"\\\\\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA A30 MIG 2g.12gb. Max memory: 11.688 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 20  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = (\n",
    "    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    ")\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vulnerable = load_dataset(\n",
    "    \"msc-smart-contract-audition/vulnerable-functions-base\",\n",
    "    split=\"train\",\n",
    "    escapechar=\"\\\\\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "The below is a code blocks which might have improper formatting. The task is to fix the formatting and remove comments which do not describe the code. Apart from formatting all of the code needs to remain the same.\n",
    "1. Ensure that the code is properly indented.\n",
    "2. Ensure that there are no leading tabs - the outer most block should start with no whitespace infront.\n",
    "3. If a line is too long break it in multiple lines.\n",
    "4. Keep all the code the same even if it seems unnecessary in the context.\n",
    "5. Remove comments which contain no information about the code.\n",
    "6. Remove any comments which contain URLs.\n",
    "\n",
    "Do not explain your changes at the end just output the formatted code block.\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Code block:\n",
    "{}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Formatted code block:\n",
    "```\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_codeblocks(description):\n",
    "    codeblocks = []\n",
    "    codeblock = \"\"\n",
    "\n",
    "    writing = False\n",
    "\n",
    "    for line in description.split(\"\\n\"):\n",
    "        if writing:\n",
    "            codeblock += line + \"\\n\"\n",
    "\n",
    "        if line.strip().startswith(\"```\"):\n",
    "            if writing:\n",
    "                codeblocks.append(codeblock)\n",
    "                yield codeblock\n",
    "                codeblock = \"\"\n",
    "                writing = False\n",
    "            else:\n",
    "                writing = True\n",
    "                codeblock += line + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame and drop 534 (too long)\n",
    "descriptions = dataset_vulnerable.to_pandas().drop(index=534)['description'].apply(lambda row: row.replace(\"\\\\n\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 10/10 [01:20<00:00,  8.08s/it]\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 100\n",
    "current_chunk = 0\n",
    "start = current_chunk * chunk_len\n",
    "end = (current_chunk + 1) * chunk_len\n",
    "\n",
    "with open(f\"cleaned-up-code-{current_chunk+1}.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, dialect=DIALECT_NAME)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow([\"code\"])\n",
    "\n",
    "    for description in tqdm(descriptions[start:end], total=chunk_len):\n",
    "        formatted_blocks = []\n",
    "        for codeblock in extract_codeblocks(description):\n",
    "            prompt = fix_template.format(codeblock)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "            output_tokens = model.generate(\n",
    "                **inputs, max_new_tokens=512, pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "            decoded_output = tokenizer.decode(\n",
    "                output_tokens[0],\n",
    "                skip_special_tokens=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            formatted_blocks.append(\n",
    "                decoded_output.split(\"Formatted code block:\\n\")[1].strip()\n",
    "            )\n",
    "\n",
    "        codeblocks = \"\\n\".join(formatted_blocks).replace(\"\\n\", \"\\\\n\")\n",
    "        writer.writerow([codeblocks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "uint inputTotalUSDValueE36;\n",
      "\n",
      "for (uint i; i < openTokenInfos.length; ) {\n",
      "    inputTotalUSDValueE36 += openTokenInfos[i].inputAmt * tokenPriceE36s[i];\n",
      "    borrowTotalUSDValueE36 += openTokenInfos[i].borrowAmt * tokenPriceE36s[i];\n",
      "    unchecked {\n",
      "        ++i;\n",
      "    }\n",
      "}\n",
      "\n",
      "positionOpenUSDValueE36 = inputTotalUSDValueE36 + borrowTotalUSDValueE36;\n",
      "netPnLE36 = positionCurUSDValueE36.toInt256() - positionOpenUSDValueE36.toInt256();\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "fix_template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "The below is a code blocks which might have improper formatting. The task is to fix the formatting and remove comments which do not describe the code. Apart from formatting all of the code needs to remain the same.\n",
    "1. Ensure that the code is properly indented.\n",
    "2. Ensure that there are no leading tabs - the outer most block should start with no whitespace infront.\n",
    "3. If a line is too long break it in multiple lines.\n",
    "4. Keep all the code the same even if it seems unnecessary in the context.\n",
    "5. Remove comments which contain no information about the code.\n",
    "6. Remove any comments which contain URLs.\n",
    "\n",
    "Do not explain your changes at the end just output the formatted code block.\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Code block:\n",
    "```\n",
    "    // File input.sol:12\n",
    "            uint inputTotalUSDValueE36;\n",
    "                for (uint i; i < openTokenInfos.length; ) {\n",
    "                 inputTotalUSDValueE36 += openTokenInfos[i].inputAmt * tokenPriceE36s[i];\n",
    "                      borrowTotalUSDValueE36 += openTokenInfos[i].borrowAmt * tokenPriceE36s[i];\n",
    "                 unchecked {\n",
    "            ++i; // See https://github.com/MSc-Smart-Contract-Audition/data-scraper/blob/main/verified/src/compile_contracts.py\n",
    "                }\n",
    "            }\n",
    "                // 1.3 calculate net pnl (including strategy users & borrow profit)\n",
    "            positionOpenUSDValueE36 = inputTotalUSDValueE36 + borrowTotalUSDValueE36;\n",
    "            netPnLE36 = positionCurUSDValueE36.toInt256() - positionOpenUSDValueE36.toInt256();\n",
    "```\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Formatted code block:\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(fix_template, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "output_tokens = model.generate(\n",
    "    **inputs, max_new_tokens=512, pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "decoded_output = tokenizer.decode(\n",
    "    output_tokens[0], skip_special_tokens=True, pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "\n",
    "print(decoded_output.split(\"Formatted code block:\\n\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                  | 5/1971 [00:25<2:48:55,  5.16s/it]\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 100\n",
    "current_chunk = 0\n",
    "start = current_chunk * chunk_len\n",
    "end = (current_chunk + 1) * chunk_len\n",
    "\n",
    "with open(f\"cleaned-up-code-{current_chunk+1}.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, dialect=DIALECT_NAME)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow([\"code\"])\n",
    "\n",
    "    for description in tqdm(descriptions[start:end], total=chunk_len):\n",
    "        formatted_blocks = []\n",
    "        for codeblock in extract_codeblocks(description):\n",
    "            prompt = fix_template.format(codeblock)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "            output_tokens = model.generate(\n",
    "                **inputs, max_new_tokens=512, pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "            decoded_output = tokenizer.decode(\n",
    "                output_tokens[0],\n",
    "                skip_special_tokens=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            formatted_blocks.append(\n",
    "                decoded_output.split(\"Formatted code block:\\n\")[1].strip()\n",
    "            )\n",
    "\n",
    "        codeblocks = \"\\n\".join(formatted_blocks).replace(\"\\n\", \"\\\\n\")\n",
    "        writer.writerow([codeblocks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../enhanced-recommendation-0.csv\n",
      "Processing ../enhanced-recommendation-1.csv\n",
      "Processing ../enhanced-recommendation-2.csv\n",
      "Processing ../enhanced-recommendation-3.csv\n",
      "Processing ../enhanced-recommendation-4.csv\n",
      "Processing ../enhanced-recommendation-5.csv\n",
      "Processing ../enhanced-recommendation-6.csv\n",
      "Processing ../enhanced-recommendation-7.csv\n",
      "Processing ../enhanced-recommendation-8.csv\n",
      "Processing ../enhanced-recommendation-9.csv\n",
      "Processing ../enhanced-recommendation-10.csv\n",
      "Processing ../enhanced-recommendation-11.csv\n",
      "Processing ../enhanced-recommendation-12.csv\n",
      "Processing ../enhanced-recommendation-13.csv\n",
      "Processing ../enhanced-recommendation-14.csv\n",
      "Processing ../enhanced-recommendation-15.csv\n",
      "Processing ../enhanced-recommendation-16.csv\n",
      "Processing ../enhanced-recommendation-17.csv\n",
      "Processing ../enhanced-recommendation-18.csv\n",
      "Processing ../enhanced-recommendation-19.csv\n",
      "Processing ../enhanced-recommendation-20.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "DIALECT_NAME = \"db_dialect\"\n",
    "csv.register_dialect(\n",
    "    DIALECT_NAME,\n",
    "    delimiter=\",\",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    escapechar=\"\\\\\",\n",
    ")\n",
    "\n",
    "# Output file name\n",
    "output_file = 'enhanced-recommendations.csv'\n",
    "header = True\n",
    "# Open the output file in write mode to create/overwrite it\n",
    "with open(output_file, 'w') as outfile:\n",
    "    # Iterate through each chunk file\n",
    "    for i in range(21):\n",
    "        input_file = f'../enhanced-recommendation-{i}.csv'\n",
    "        print(f\"Processing {input_file}\")\n",
    "\n",
    "        # Read the input CSV file\n",
    "        df = pd.read_csv(input_file, dialect=DIALECT_NAME)\n",
    "\n",
    "        # For the first file, include the header\n",
    "        df.to_csv(output_file, mode='a', index=False, header=header)\n",
    "        header = False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
